An implementation method of Adapter-tuning, which only provides the following ideas. The details can be slightly modified depending on the situation.


Here are some additional model layers:
GPT-2 Small: 12 GPT2Blocks, approximately 117 million parameters.
GPT-2 Medium: 24 GPT2Blocks, approximately 348 million parameters.
GPT-2 Large: 36 GPT2Blocks, approximately 755 million parameters.
GPT-2 XL (also known as Extra Large): 48 GPT2Blocks, approximately 1.554 billion parameters.

RoBERTa Base: 12 RobertaLayer, with a total of about 125 million parameters.
RoBERTa Large: 24 RobertaLayer, with a total of about 355 million parameters.
