"""Keyword optimization middleware
Use Qwen AI to optimize the search terms generated by the Agent into keywords more suitable for public opinion database queries"""

from openai import OpenAI
import json
import sys
import os
from typing import List, Dict, Any
from dataclasses import dataclass

# Add project root directory to Python path to import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import settings
from loguru import logger

# Add utils directory to Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
utils_dir = os.path.join(root_dir, 'utils')
if utils_dir not in sys.path:
    sys.path.append(utils_dir)

from retry_helper import with_graceful_retry, SEARCH_API_RETRY_CONFIG

@dataclass
class KeywordOptimizationResponse:
    """Keyword Optimization Response"""
    original_query: str
    optimized_keywords: List[str]
    reasoning: str
    success: bool
    error_message: str = ""

class KeywordOptimizer:
    """keyword optimizer
    Use the silicon-based flow Qwen3 model to optimize the search terms generated by the Agent into keywords that are closer to real public opinion."""
    
    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None):
        """Initialize keyword optimizer
        
        Args:
            api_key: Silicon Flow API key, if not provided it will be read from the configuration file
            base_url: interface base address, the SiliconFlow address provided by the configuration file is used by default"""
        self.api_key = api_key or settings.KEYWORD_OPTIMIZER_API_KEY

        if not self.api_key:
            raise ValueError("Silicon Flow API key not found, please set KEYWORD_OPTIMIZER_API_KEY in config.py")

        self.base_url = base_url or settings.KEYWORD_OPTIMIZER_BASE_URL

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.model = model_name or settings.KEYWORD_OPTIMIZER_MODEL_NAME
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """Optimize search keywords
        
        Args:
            original_query: original search query generated by Agent
            context: additional contextual information (such as paragraph title, content description, etc.)
            
        Returns:
            KeywordOptimizationResponse: optimized keyword list"""
        logger.info(f"üîç Keyword optimization middleware: processing query '{original_query}'")
        
        try:
            # Build optimization prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(original_query, context)
            
            # Call Qwen API
            response = self._call_qwen_api(system_prompt, user_prompt)
            
            if response["success"]:
                # Parse response
                content = response["content"]
                try:
                    # Try to parse a JSON formatted response
                    if content.strip().startswith('{'):
                        parsed = json.loads(content)
                        keywords = parsed.get("keywords", [])
                        reasoning = parsed.get("reasoning", "")
                    else:
                        # If it is not in JSON format, try to extract keywords from the text
                        keywords = self._extract_keywords_from_text(content)
                        reasoning = content
                    
                    # Verify keyword quality
                    validated_keywords = self._validate_keywords(keywords)
                    
                    logger.info(
                        f"‚úÖ Optimization successful: {len(validated_keywords)} keywords" +
                        ("" if not validated_keywords else "\n" +
                         "\n".join([f"   {i}. '{k}'" for i, k in enumerate(validated_keywords, 1)]))
                    )
                        
                    
                    
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=validated_keywords,
                        reasoning=reasoning,
                        success=True
                    )
                
                except Exception as e:
                    logger.exception(f"‚ö†Ô∏è Failed to parse response, use alternative: {str(e)}")
                    # Alternative: Extract keywords from original query
                    fallback_keywords = self._fallback_keyword_extraction(original_query)
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=fallback_keywords,
                        reasoning="API response parsing failed, use alternate keyword extraction",
                        success=True
                    )
            else:
                logger.error(f"‚ùå API call failed: {response['error']}")
                # Use backup plan
                fallback_keywords = self._fallback_keyword_extraction(original_query)
                return KeywordOptimizationResponse(
                    original_query=original_query,
                    optimized_keywords=fallback_keywords,
                    reasoning="API call failed, use alternate keyword extraction",
                    success=True,
                    error_message=response['error']
                )
                
        except Exception as e:
            logger.error(f"‚ùå Keyword optimization failed: {str(e)}")
            # final alternative
            fallback_keywords = self._fallback_keyword_extraction(original_query)
            return KeywordOptimizationResponse(
                original_query=original_query,
                optimized_keywords=fallback_keywords,
                reasoning="System error, use alternate keyword extraction",
                success=False,
                error_message=str(e)
            )
    
    def _build_system_prompt(self) -> str:
        """Build system prompt"""
        return """You are a professional public opinion data mining expert. Your job is to refine user-supplied search queries into keywords that are more suitable for finding in social media opinion databases.

**Core Principles**:
1. **Language close to netizens**: Use vocabulary that ordinary netizens would use on social media
2. **Avoid jargon**: Do not use"ËàÜÊÉÖ"„ÄÅ"‰º†Êí≠"„ÄÅ"ÂÄæÂêë"„ÄÅ"Â±ïÊúõ"and other official vocabulary
3. **Concise and specific**: Each keyword should be very concise and clear to facilitate database matching.
4. **Emotional rich**: Contains emotional expression words commonly used by netizens
5. **Quantity Control**: Provide a minimum of 10 keywords and a maximum of 20 keywords
6. **Avoid duplication**: Don‚Äôt stray from the topic of the initial query

**Important reminder**: Each keyword must be an indivisible independent entry, and it is strictly prohibited to include spaces within the entry. For example, one should use"Èõ∑ÂÜõÁè≠‰∫âËÆÆ"rather than wrong"Èõ∑ÂÜõÁè≠ ‰∫âËÆÆ".


**Output format**:
Please return the results in JSON format:
{"keywords": ["ÂÖ≥ÈîÆËØç1", "ÂÖ≥ÈîÆËØç2", "ÂÖ≥ÈîÆËØç3"],
    "reasoning": "ÈÄâÊã©Ëøô‰∫õÂÖ≥ÈîÆËØçÁöÑÁêÜÁî±"}

**Example**:
Input:"Ê≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖÁÆ°ÁêÜ Êú™Êù•Â±ïÊúõ ÂèëÂ±ïË∂ãÂäø"Output:
{"keywords": ["Ê≠¶Â§ß", "Ê≠¶Ê±âÂ§ßÂ≠¶", "Â≠¶Ê†°ÁÆ°ÁêÜ", "Â§ßÂ≠¶", "ÊïôËÇ≤"],
    "reasoning": "ÈÄâÊã©'Ê≠¶Â§ß'Âíå'Ê≠¶Ê±âÂ§ßÂ≠¶'‰Ωú‰∏∫Ê†∏ÂøÉËØçÊ±áÔºåËøôÊòØÁΩëÊ∞ëÊúÄÂ∏∏‰ΩøÁî®ÁöÑÁß∞ÂëºÔºõ'Â≠¶Ê†°ÁÆ°ÁêÜ'ÊØî'ËàÜÊÉÖÁÆ°ÁêÜ'Êõ¥Ë¥¥ËøëÊó•Â∏∏Ë°®ËææÔºõÈÅøÂÖç‰ΩøÁî®'Êú™Êù•Â±ïÊúõ'„ÄÅ'ÂèëÂ±ïË∂ãÂäø'Á≠âÁΩëÊ∞ëÂæàÂ∞ë‰ΩøÁî®ÁöÑ‰∏ì‰∏öÊúØËØ≠"
}"""

    def _build_user_prompt(self, original_query: str, context: str) -> str:
        """Build user prompt"""
        prompt = f"Please optimize the following search query into keywords suitable for public opinion database query:\n\nOriginal query: {original_query}"
        
        if context:
            prompt += f"\n\nContext information: {context}"
        
        prompt += "\n\nRemember: use words that netizens actually use on social media and avoid official terms and professional vocabulary."
        
        return prompt
    
    @with_graceful_retry(SEARCH_API_RETRY_CONFIG, default_return={"success": False, "error": "Keyword optimization service is temporarily unavailable"})
    def _call_qwen_api(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """Call Qwen API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
            )

            if response.choices:
                content = response.choices[0].message.content
                return {"success": True, "content": content}
            else:
                return {"success": False, "error": "API return format exception"}
        except Exception as e:
            return {"success": False, "error": f"API call exception: {str(e)}"}
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """Extract keywords from text (used when JSON parsing fails)"""
        # Simple keyword extraction logic
        lines = text.split('\n')
        keywords = []
        
        for line in lines:
            line = line.strip()
            # Find possible keywords
            if 'Ôºö' in line or ':' in line:
                parts = line.split('Ôºö') if 'Ôºö' in line else line.split(':')
                if len(parts) > 1:
                    potential_keywords = parts[1].strip()
                    # Try segmenting keywords
                    if '„ÄÅ' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split('„ÄÅ')])
                    elif ',' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split(',')])
                    else:
                        keywords.append(potential_keywords)
        
        # If not found, try other methods
        if not keywords:
            # Find content in quotes
            import re
            quoted_content = re.findall(r'["""\'](.*?)["""\']', text)
            keywords.extend(quoted_content)
        
        # Clean and validate keywords
        cleaned_keywords = []
        for keyword in keywords[:20]:  # Up to 20
            keyword = keyword.strip().strip('"\'""''')
            if keyword and len(keyword) <= 20: # Reasonable length
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords[:20]
    
    def _validate_keywords(self, keywords: List[str]) -> List[str]:"r]:
        """È™åËØÅÂíåÊ∏ÖÁêÜÂÖ≥ÈîÆËØç"""validated = []
        
        # Bad keywords (too professional or official)
        bad_keywords = {
            'Attitude analysis', 'Public reaction', 'Emotional tendencies',
            'Future Outlook', 'Development Trend', 'Strategic Planning', 'Policy Orientation', 'Management Mechanism'
        }
        
        for keyword in keywords:
            if isinstance(keyword, str):
                keyword = keyword.strip().strip('"rd = keyword.strip().strip('"\'""''')
                
                # Basic verification
                if (keyword and 
                    len(keyword) <= 20 and 
                    len(keyword) >= 1 and
                    not any(bad_word in keyword for bad_word in bad_keywords)):
                    validated.append(keyword)
        
        return validated[:20]  # Return up to 20 keywords
    
    def _fallback_keyword_extraction(self, original_query: str) -> List[str]:
        """Alternate keyword extraction solution"""
        # Simple keyword extraction logic
        # Remove common useless words
        stop_words = {'„ÄÅ'}
        
        # split query
        import re
        # Split by spaces and punctuation
        tokens = re.split(r'[\sÔºå„ÄÇÔºÅÔºüÔºõÔºö„ÄÅ]+', original_query)
        
        keywords = []
        for token in tokens:
            token = token.strip()
            if token and token not in stop_words and len(token) >= 2:
                keywords.append(token)
        
        # If there are no valid keywords, use the first word of the original query
        if not keywords:
            first_word = original_query.split()[0] if original_query.split() else original_query
            keywords = [first_word] if first_word else ["Popular"]
        
        return keywords[:20]

# global instance
keyword_optimizer = KeywordOptimizer()
