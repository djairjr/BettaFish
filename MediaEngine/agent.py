"""Deep Search Agent main class
Integrate all modules to achieve a complete in-depth search process"""

import json
import os
import re
from datetime import datetime
from typing import Optional, Dict, Any, List
from loguru import logger
from .llms import LLMClient
from .nodes import (
    ReportStructureNode,
    FirstSearchNode, 
    ReflectionNode,
    FirstSummaryNode,
    ReflectionSummaryNode,
    ReportFormattingNode
)
from .state import State
from .tools import BochaMultimodalSearch, BochaResponse, AnspireAISearch, AnspireResponse
from .utils import settings, Settings, format_search_results_for_prompt


class DeepSearchAgent:
    """Deep Search Agent main class"""
    
    def __init__(self, config: Optional[Settings] = None):
        """Initialize Deep Search Agent
        
        Args:
            config: configuration object, automatically loaded if not provided"""
        self.config = config or settings
        
        # Initialize LLM client
        self.llm_client = self._initialize_llm()
        
        # Initialize the search toolset
        self.search_agency = BochaMultimodalSearch(api_key=(self.config.BOCHA_API_KEY or self.config.BOCHA_WEB_SEARCH_API_KEY))
        
        # Initialize node
        self._initialize_nodes()
        
        # state
        self.state = State()
        
        # Make sure the output directory exists
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        logger.info(f"Media Agent has been initialized")
        logger.info(f"Using LLM: {self.llm_client.get_model_info()}")
        logger.info(f"Search toolset: BochaMultimodalSearch (supports 5 multimodal search tools)")
    
    def _initialize_llm(self) -> LLMClient:
        """Initialize LLM client"""
        return LLMClient(
            api_key=(self.config.MEDIA_ENGINE_API_KEY or self.config.MINDSPIDER_API_KEY),
            model_name=(self.config.MEDIA_ENGINE_MODEL_NAME or self.config.MINDSPIDER_MODEL_NAME),
            base_url=(self.config.MEDIA_ENGINE_BASE_URL or self.config.MINDSPIDER_BASE_URL),
        )
    
    def _initialize_nodes(self):
        """Initialize processing node"""
        self.first_search_node = FirstSearchNode(self.llm_client)
        self.reflection_node = ReflectionNode(self.llm_client)
        self.first_summary_node = FirstSummaryNode(self.llm_client)
        self.reflection_summary_node = ReflectionSummaryNode(self.llm_client)
        self.report_formatting_node = ReportFormattingNode(self.llm_client)
    
    def _validate_date_format(self, date_str: str) -> bool:
        """Verify that the date format is YYYY-MM-DD
        
        Args:
            date_str: date string
            
        Returns:
            Is it a valid format?"""
        if not date_str:
            return False
        
        # Check format
        pattern = r'^\d{4}-\d{2}-\d{2}$'
        if not re.match(pattern, date_str):
            return False
        
        # Check if the date is valid
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return True
        except ValueError:
            return False
    
    def execute_search_tool(self, tool_name: str, query: str, **kwargs) -> BochaResponse:
        """Execute the specified search tool
        
        Args:
            tool_name: tool name, optional value:
                -"comprehensive_search": Comprehensive comprehensive search (default)
                -"web_search_only": Pure web search
                -"search_for_structured_data": Structured data query
                -"search_last_24_hours": Latest information within 24 hours
                -"search_last_week": Information of the week
            query: search query
            **kwargs: additional parameters (such as max_results)
            
        Returns:
            BochaResponse object"""
        logger.info(f"→ Execute search tool: {tool_name}")
        
        if tool_name == "comprehensive_search":
            max_results = kwargs.get("max_results", 10)
            return self.search_agency.comprehensive_search(query, max_results)
        elif tool_name == "web_search_only":
            max_results = kwargs.get("max_results", 15)
            return self.search_agency.web_search_only(query, max_results)
        elif tool_name == "search_for_structured_data":
            return self.search_agency.search_for_structured_data(query)
        elif tool_name == "search_last_24_hours":
            return self.search_agency.search_last_24_hours(query)
        elif tool_name == "search_last_week":
            return self.search_agency.search_last_week(query)
        else:
            logger.info(f"⚠️ Unknown search tool: {tool_name}, use the default comprehensive search")
            return self.search_agency.comprehensive_search(query)
    
    def research(self, query: str, save_report: bool = True) -> str:
        """Perform in-depth research
        
        Args:
            query: research query
            save_report: whether to save the report to a file
            
        Returns:
            Final report content"""
        logger.info(f"\n{'='*60}")
        logger.info(f"Start in-depth research: {query}")
        logger.info(f"{'='*60}")
        
        try:
            # Step 1: Generate report structure
            self._generate_report_structure(query)
            
            # Step 2: Process each paragraph
            self._process_paragraphs()
            
            # Step 3: Generate final report
            final_report = self._generate_final_report()
            
            # Step 4: Save report
            if save_report:
                self._save_report(final_report)
            
            logger.info(f"\n{'='*60}")
            logger.info("In-depth research completed!")
            logger.info(f"{'='*60}")
            
            return final_report
            
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"An error occurred during research: {str(e)} \nError stack: {error_traceback}")
            raise e
    
    def _generate_report_structure(self, query: str):
        """Generate report structure"""
        logger.info(f"\n[Step 1] Generate report structure...")
        
        # Create report structure node
        report_structure_node = ReportStructureNode(self.llm_client, query)
        
        # Generate structure and update status
        self.state = report_structure_node.mutate_state(state=self.state)
        
        _message = f"The report structure has been generated, with {len(self.state.paragraphs)} paragraphs in total:"
        for i, paragraph in enumerate(self.state.paragraphs, 1):
            _message += f"\n  {i}. {paragraph.title}"
        logger.info(_message)
    
    def _process_paragraphs(self):
        """process all paragraphs"""
        total_paragraphs = len(self.state.paragraphs)
        
        for i in range(total_paragraphs):
            logger.info(f"\n[Step 2.{i+1}] Process paragraphs: {self.state.paragraphs[i].title}")
            logger.info("-" * 50)
            
            # Initial search and summary
            self._initial_search_and_summary(i)
            
            # reflective cycle
            self._reflection_loop(i)
            
            # Mark paragraph complete
            self.state.paragraphs[i].research.mark_completed()
            
            progress = (i + 1) / total_paragraphs * 100
            logger.info(f"Paragraph processing completed ({progress:.1f}%)")
    
    def _initial_search_and_summary(self, paragraph_index: int):
        """Perform initial search and summary"""
        paragraph = self.state.paragraphs[paragraph_index]
        
        # Prepare search input
        search_input = {
            "title": paragraph.title,
            "content": paragraph.content
        }
        
        # Generate search queries and tool selections
        logger.info("- Generate search queries...")
        search_output = self.first_search_node.run(search_input)
        search_query = search_output["search_query"]
        search_tool = search_output.get("search_tool", "comprehensive_search")  # Default tool
        reasoning = search_output["reasoning"]
        
        logger.info(f"- Search query: {search_query}")
        logger.info(f"- Selected tool: {search_tool}")
        logger.info(f"- Reasoning: {reasoning}")
        
        # Perform a search
        logger.info("- Perform a web search...")
        
        # Handle special parameters (new toolset does not require date parameter handling)
        search_kwargs = {}
        if search_tool in ["comprehensive_search", "web_search_only"]:
            # These tools support the max_results parameter
            search_kwargs["max_results"] = 10
        
        search_response = self.execute_search_tool(search_tool, search_query, **search_kwargs)
        
        # Convert to compatible format
        search_results = []
        if search_response and search_response.webpages:
            # Each search tool has its specific number of results, here we take the top 10 as the upper limit
            max_results = min(len(search_response.webpages), 10)
            for result in search_response.webpages[:max_results]:
                search_results.append({
                    'title': result.name,
                    'url': result.url,
                    'content': result.snippet,
                    'score': None,  # Bocha API does not provide score
                    'raw_content': result.snippet,
                    'published_date': result.date_last_crawled  # Use crawl date
                })
        
        if search_results:
            _message = f"- {len(search_results)} search results found" 
            for j, result in enumerate(search_results, 1):
                date_info = f"(Published in: {result.get('published_date', 'N/A')})" if result.get('published_date') else ""
                _message += f"\n    {j}. {result['title'][:50]}...{date_info}"
            logger.info(_message)
        else:
            logger.info("- No search results found")
        
        # Update search history in status
        paragraph.research.add_search_results(
            search_query,
            search_results,
            search_tool=search_tool,
            paragraph_title=paragraph.title,
        )
        
        # Generate initial summary
        logger.info("- Generate initial summary...")
        summary_input = {
            "title": paragraph.title,
            "content": paragraph.content,
            "search_query": search_query,
            "search_results": format_search_results_for_prompt(
                search_results, self.config.SEARCH_CONTENT_MAX_LENGTH
            )
        }
        
        # update status
        self.state = self.first_summary_node.mutate_state(
            summary_input, self.state, paragraph_index
        )
        
        logger.info("- Initial summary completed")
    
    def _reflection_loop(self, paragraph_index: int):
        """Execute a reflective cycle"""
        paragraph = self.state.paragraphs[paragraph_index]
        
        for reflection_i in range(self.config.MAX_REFLECTIONS):
            logger.info(f"- reflection {reflection_i + 1}/{self.config.MAX_REFLECTIONS}...")
            
            # Prepare reflective input
            reflection_input = {
                "title": paragraph.title,
                "content": paragraph.content,
                "paragraph_latest_state": paragraph.research.latest_summary
            }
            
            # Generate reflective search queries
            reflection_output = self.reflection_node.run(reflection_input)
            search_query = reflection_output["search_query"]
            search_tool = reflection_output.get("search_tool", "comprehensive_search")  # Default tool
            reasoning = reflection_output["reasoning"]
            
            logger.info(f"Reflection query: {search_query}")
            logger.info(f"Selected tool: {search_tool}")
            logger.info(f"reflective reasoning: {reasoning}")
            
            # Perform a reflective search
            # Handle special parameters
            search_kwargs = {}
            if search_tool in ["comprehensive_search", "web_search_only"]:
                # These tools support the max_results parameter
                search_kwargs["max_results"] = 10
            
            search_response = self.execute_search_tool(search_tool, search_query, **search_kwargs)
            
            # Convert to compatible format
            search_results = []
            if search_response and search_response.webpages:
                # Each search tool has its specific number of results, here we take the top 10 as the upper limit
                max_results = min(len(search_response.webpages), 10)
                for result in search_response.webpages[:max_results]:
                    search_results.append({
                        'title': result.name,
                        'url': result.url,
                        'content': result.snippet,
                        'score': None,  # Bocha API does not provide score
                        'raw_content': result.snippet,
                        'published_date': result.date_last_crawled
                    })
            
            if search_results:
                _message = f"Found {len(search_results)} reflection search results"
                for j, result in enumerate(search_results, 1):
                    date_info = f"(Published in: {result.get('published_date', 'N/A')})" if result.get('published_date') else ""
                    _message += f"\n      {j}. {result['title'][:50]}...{date_info}"
                logger.info(_message)
            else:
                logger.info("No reflection search results found")
            
            # Update search history
            paragraph.research.add_search_results(
                search_query,
                search_results,
                search_tool=search_tool,
                paragraph_title=paragraph.title,
            )
            
            # Generate reflection summaries
            reflection_summary_input = {
                "title": paragraph.title,
                "content": paragraph.content,
                "search_query": search_query,
                "search_results": format_search_results_for_prompt(
                    search_results, self.config.SEARCH_CONTENT_MAX_LENGTH
                ),
                "paragraph_latest_state": paragraph.research.latest_summary
            }
            
            # update status
            self.state = self.reflection_summary_node.mutate_state(
                reflection_summary_input, self.state, paragraph_index
            )
            
            logger.info(f"Reflection {reflection_i + 1} completed")
    
    def _generate_final_report(self) -> str:
        """Generate final report"""
        logger.info(f"\n[Step 3] Generate final report...")
        
        # Prepare reporting data
        report_data = []
        for paragraph in self.state.paragraphs:
            report_data.append({
                "title": paragraph.title,
                "paragraph_latest_state": paragraph.research.latest_summary
            })
        
        # Format reports
        try:
            final_report = self.report_formatting_node.run(report_data)
        except Exception as e:
            logger.info(f"LLM format failed, use fallback method: {str(e)}")
            final_report = self.report_formatting_node.format_report_manually(
                report_data, self.state.report_title
            )
        
        # update status
        self.state.final_report = final_report
        self.state.mark_completed()
        
        logger.info("Final report generation completed")
        return final_report
    
    def _save_report(self, report_content: str):
        """Save report to file"""
        # Generate file name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_safe = "".join(c for c in self.state.query if c.isalnum() or c in (' ', '-', '_')).rstrip()
        query_safe = query_safe.replace(' ', '_')[:30]
        
        filename = f"deep_search_report_{query_safe}_{timestamp}.md"
        filepath = os.path.join(self.config.OUTPUT_DIR, filename)
        
        # save report
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"Report saved to: {filepath}")
        
        # Save state (if configuration allows)
        if self.config.SAVE_INTERMEDIATE_STATES:
            state_filename = f"state_{query_safe}_{timestamp}.json"
            state_filepath = os.path.join(self.config.OUTPUT_DIR, state_filename)
            self.state.save_to_file(state_filepath)
            logger.info(f"State saved to: {state_filepath}")
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """Get progress summary"""
        return self.state.get_progress_summary()
    
    def load_state(self, filepath: str):
        """Load status from file"""
        self.state = State.load_from_file(filepath)
        logger.info(f"Status loaded from {filepath}")
    
    def save_state(self, filepath: str):
        """Save state to file"""
        self.state.save_to_file(filepath)
        logger.info(f"Status saved to {filepath}")

class AnspireSearchAgent(DeepSearchAgent):
    """Call the Deep Search Agent of Anspire search engine"""
    
    def __init__(self, config: Settings | None = None):
        self.config = config or settings
        
        # Initialize LLM client
        self.llm_client = self._initialize_llm()
        
        # Initialize the search toolset
        self.search_agency = AnspireAISearch(api_key=self.config.ANSPIRE_API_KEY)

        # Initialize node
        self._initialize_nodes()
        
        # state
        self.state = State()
        
        # Make sure the output directory exists
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        logger.info(f"Media Agent has been initialized")
        logger.info(f"Using LLM: {self.llm_client.get_model_info()}")
        logger.info(f"Search toolset: AnspireSearch")

    def execute_search_tool(self, tool_name: str, query: str, **kwargs) -> AnspireResponse:
        # TODO: Perform a search using the Anspire search tool
        """Execute the specified search tool
        
        Args:
            tool_name: tool name, optional value:
                -"comprehensive_search": Comprehensive comprehensive search (default)
                -"search_last_24_hours": Latest information within 24 hours
                -"search_last_week": Information of the week
            query: search query
            **kwargs: additional parameters (such as max_results)
            
        Returns:
            AnspireResponse object"""
        logger.info(f"→ Execute search tool: {tool_name}")
        
        if tool_name == "comprehensive_search":
            max_results = kwargs.get("max_results", 10)
            return self.search_agency.comprehensive_search(query, max_results)
        elif tool_name == "search_last_24_hours":
            return self.search_agency.search_last_24_hours(query)
        elif tool_name == "search_last_week":
            return self.search_agency.search_last_week(query)
        else:
            logger.info(f"⚠️ Unknown search tool: {tool_name}, use the default comprehensive search")
            return self.search_agency.comprehensive_search(query)


def create_agent(config_file: Optional[str] = None) -> DeepSearchAgent:
    """Convenience functions for creating Deep Search Agent instances
    
    Args:
        config_file: configuration file path
        
    Returns:
        DeepSearchAgent instance"""
    settings = Settings()
    if settings.SEARCH_TOOL_TYPE == "AnspireAPI":
        return AnspireSearchAgent(settings)
    return DeepSearchAgent(settings)
